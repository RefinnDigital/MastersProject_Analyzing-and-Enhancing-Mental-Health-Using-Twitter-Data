# -*- coding: utf-8 -*-
"""LSTMMODEL_dissertation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15onzHZJTfGDdviBsbA_EcPFcwqs9uarl

# SETUP ENVIRONMENT
"""

! pip install tweepy
! pip install NRCLex
! pip install preprocessor
! pip install textblob
! pip install transformers
! pip install pycaret
! pip install langdetect

import csv
import re
import string
import pickle
import numpy as np
import pandas as pd
import nltk
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup
from textblob.blob import TextBlob
from nrclex import NRCLex
from numpy import array, asarray, zeros
from tweepy import *
from keras.preprocessing.text import one_hot, Tokenizer
from keras.utils import pad_sequences
from keras.models import Sequential
from keras.layers import Activation, Dropout, Dense, Flatten, GlobalMaxPooling1D, Embedding, LSTM
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from keras.optimizers import Nadam
import pycaret
import transformers
from transformers import AutoModel, BertTokenizerFast
import torch
import torch.nn as nn

# specifing GPU
device = torch.device("cuda")

# NLTK downloads
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')  # download wordnet corpus for lemmatization

"""# LOAD DATASET
check data summary
drop duplicates

"""

FINAL_DATA = pd.read_csv("/content/SOCIOMENTAL_DATA.csv")

FINAL_DATA.head()

FINAL_DATA.info()

FINAL_DATA = FINAL_DATA.dropna()

FINAL_DATA.drop_duplicates(inplace=True)

FINAL_DATA.shape

FINAL_DATA.info()

"""# PREPROCESSING
Loading the libraries
"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup

nltk.download('wordnet')
nltk.download('stopwords')

def clean_text(text, lemmatizer=WordNetLemmatizer(), stop_words=set(stopwords.words('english'))):
    # Removing HTML tags

    soup = BeautifulSoup(text, "html.parser")
    no_html_review = soup.get_text().lower()

    abbreviation_dict = {
        "dm": "direct message",
        "pm": "private message",
        "smh": "shaking my head",
        "tbh": "to be honest",
        "idk": "I don't know",
        "omg": "oh my god",
        "lol": "laughing out loud",
        "ofc": "of course",
        "fr": "for real",
        "ppl": "people",
        "rn": "right now",
        "tbtb": "taken by the best",
    }

    # Tokenization and abbreviation replacement
    stop_words = set(stopwords.words('english'))
    stop_words.update(("ifs","fill","make","like","every","find"))
    words = word_tokenize(no_html_review)
    words = [abbreviation_dict.get(word, word) for word in words]

    # Lemmatizing words and filtering
    clean_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word.isalpha()]

    return " ".join(clean_words)

# Drop rows where 'Tweet' is not a string
FINAL_DATA = FINAL_DATA[FINAL_DATA['Tweet'].apply(lambda x: isinstance(x, str))]

FINAL_DATA = FINAL_DATA.applymap(lambda x: x.lower() if isinstance(x, str) else x)

FINAL_DATA['Tweet'] =FINAL_DATA['Tweet'].str.replace('rt', '')

FINAL_DATA['Tweet'] =FINAL_DATA['Tweet'].str.replace('co', '')

# Commented out IPython magic to ensure Python compatibility.
# code for cleaning dataset appended in the list above
# %time
FINAL_DATA['Tweet'] = FINAL_DATA['Tweet'].apply(clean_text)

FINAL_DATA.head(2)

FINAL_DATA['Tweet']

# Extend the set of stop words
stop_words = set(stopwords.words('english'))




def clean_tweet(tweet):
    # Remove multiple spaces
    tweet = re.sub(r'\s+', ' ', tweet)

    # Remove words with 2 or fewer letters
    tweet = re.sub(r'\b\w{1,2}\b', '', tweet)

    # Remove RT
    tweet = re.sub(r'\bRT\b', '', tweet)

    # Tokenize and remove stop words
    words = word_tokenize(tweet)
    words = [word for word in words if word not in stop_words]

    # Re-construct the tweet from cleaned words
    return ' '.join(words)

# Assuming FINAL_DATA is a pandas DataFrame
FINAL_DATA["Tweet"] = FINAL_DATA["Tweet"].apply(clean_tweet)

FINAL_DATA.head()

FINAL_DATA.isnull().values.any()

"""### SENTIMENT ANALYSIS

### EMOTIONAL LEXICONS
"""

# Function to retrieve the emotion with the highest score
def get_top_emotion(emotion_object):
    emotions = emotion_object.raw_emotion_scores
    if not emotions:
        return 'neutral'
    else:
        return max(emotions, key=emotions.get)

# Creating a new column in the DataFrame to hold the NRCLex sentiment analysis
FINAL_DATA['emotions'] = FINAL_DATA['Tweet'].apply(lambda x: NRCLex(x))

# Getting the top emotion for each tweet
FINAL_DATA['top_emotion'] = FINAL_DATA['emotions'].apply(get_top_emotion)

# Getting the fraction of each emotional type in each tweet for further analysis
FINAL_DATA['emotion_fractions'] = FINAL_DATA['emotions'].apply(lambda x: x.affect_frequencies)

FINAL_DATA

FINAL_DATA.info()

final_data = FINAL_DATA

emotional_count = final_data["top_emotion"].value_counts()

emotional_count

# Define a list of colors for each emotion
colors = ['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'brown', 'grey', 'cyan']

# Plot the bar chart
plt.bar(emotional_count.index, emotional_count.values, color=colors)

plt.xlabel('Emotion')
plt.ylabel('Count')
plt.title('Count of Top Emotions')
plt.xticks(rotation=45)
plt.show()

final_data = FINAL_DATA
final_data.head()

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud




# Define your classes
classes = [
    "anger",
    "negative",
    "positive",
    "fear",
    "anticipation",
    "sadness",
    "joy",
    "disgust",
    "trust",
    "surprise",
    "neutral"
]

# Generate a word cloud for each class
for class_ in classes:
    # Filter the data for the current class
    class_data = FINAL_DATA[FINAL_DATA['top_emotion'] == class_]

    # Combine all the tweets into a single text
    text = ' '.join(class_data['Tweet'])

    # Generate the word cloud
    wordcloud = WordCloud(width=800, height=400).generate(text)

    # Plot the word cloud
    plt.figure(figsize=(10, 7))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Word Cloud for {class_}')
    plt.show()

#wordcloud for twitter data
from wordcloud import WordCloud
def create_wordcloud(data, feature):
  all_words = ' '.join([text for text in data[feature]])
  wordcloud = WordCloud(background_color='white', colormap='jet', width=800, height=500, random_state=21, max_font_size=110).generate(all_words)
  plt.figure(figsize=(10, 7))
  plt.imshow(wordcloud, interpolation="bilinear")
  plt.axis('off')
  plt.show()

create_wordcloud(data=final_data, feature='Tweet')
plt.savefig('combine_mental_image.png')

import re
from collections import Counter
import matplotlib.pyplot as plt

def visualize_top_words(data, top_N=20):
    # Combine all tweets
    all_tweets = ' '.join(final_data['Tweet'])

    # Tokenization
    words = re.findall(r'\w+', all_tweets.lower())

    # Get word frequencies
    word_freq = Counter(words)
    print(word_freq.most_common())

    # Sort word frequencies
    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

    # Take the top N words for visualization
    words, frequencies = zip(*sorted_word_freq[:top_N])

    # Plot
    plt.figure(figsize=(10, 5))
    plt.plot(words, frequencies, marker='o', linestyle='-', color='c')
    plt.xticks(rotation=45)
    plt.xlabel('Words')
    plt.ylabel('Frequency')
    plt.title(f'Top {top_N} Word Frequencies')
    #plt.grid(True)
    plt.tight_layout()
    plt.show()

visualize_top_words(final_data)
plt.savefig('word_freq.png')

import re
from collections import Counter
import matplotlib.pyplot as plt

def visualize_top_words(data, top_N=20):
    # Combine all tweets
    all_tweets = ' '.join(data['Tweet'])

    # Tokenization
    words = re.findall(r'\w+', all_tweets.lower())

    # Get word frequencies
    word_freq = Counter(words)
    print(word_freq.most_common())

    # Sort word frequencies
    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

    # Take the top N words for visualization
    words, frequencies = zip(*sorted_word_freq[:top_N])

    # Define colors for each word
    colors = plt.cm.tab20(range(len(words)))

    # Plot
    plt.figure(figsize=(10, 5))
    bars = plt.bar(words, frequencies, color=colors)
    plt.xticks(rotation=45)
    plt.xlabel('Words')
    plt.ylabel('Frequency')
    plt.title(f'Top {top_N} Word Frequencies')
    plt.tight_layout()

    # Add a legend for the colors
    plt.legend(bars, words, loc='upper right', bbox_to_anchor=(1.25, 1))

    plt.show()

# Example usage:
visualize_top_words(final_data, top_N=20)
plt.savefig('freq_image.png')

final_data

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

#Ensuring our labels are correctly encoded as integers.
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
final_data['top_emotion_int'] = encoder.fit_transform(final_data['top_emotion'])

from sklearn.metrics import accuracy_score, classification_report

def tokenize_tweets(tweets, max_words=50000):
    """Tokenize the tweets and return the tokenizer and sequences."""
    tokenizer = Tokenizer(num_words=max_words, split=" ")
    tokenizer.fit_on_texts(tweets)
    sequences = tokenizer.texts_to_sequences(tweets)
    return tokenizer, sequences

import requests
import zipfile
import io

url = "http://nlp.stanford.edu/data/glove.twitter.27B.zip"
response = requests.get(url)
z = zipfile.ZipFile(io.BytesIO(response.content))
z.extractall()

def load_glove_embeddings(glove_file, embedding_dim=200):
    """Load GloVe embeddings into a dictionary."""
    embeddings = {}
    with open(glove_file, 'r', encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            vector = np.asarray(values[1:embedding_dim+1], dtype='float32')
            embeddings[word] = vector
    return embeddings

def create_embedding_matrix(tokenizer, embeddings, embedding_dim=200, max_words=50000):
    """Create an embedding matrix using pre-trained word vectors."""
    word_index = tokenizer.word_index
    num_words = min(len(word_index) + 1, max_words)
    embedding_matrix = np.zeros((num_words, embedding_dim))

    for word, i in word_index.items():
        if i >= max_words:
            continue
        vector = embeddings.get(word)
        if vector is not None:
            embedding_matrix[i] = vector
    return embedding_matrix

def build_lstm_model(embedding_matrix, num_classes, embedding_dim=200, lstm_units=300, input_length=280):
    """Build and compile the LSTM model."""
    model = Sequential()
    model.add(Embedding(len(embedding_matrix), embedding_dim, weights=[embedding_matrix],
                        input_length=input_length, trainable=False))
    model.add(LSTM(lstm_units, dropout=0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

def train_and_evaluate(model, X_train, y_train_encoded, X_val, y_val_encoded, batch_size=128, epochs=20):
    """Train the model and evaluate on validation set using early stopping."""
    early_stop = EarlyStopping(monitor='val_loss', patience=3)
    history = model.fit(X_train, y_train_encoded, validation_data=(X_val, y_val_encoded),
                        batch_size=batch_size, epochs=epochs, shuffle=True, callbacks=[early_stop])
    return history

# loading the GloVe file
glove_file = "glove.twitter.27B.200d.txt"
embeddings = load_glove_embeddings(glove_file)

from sklearn.preprocessing import LabelBinarizer
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, classification_report

mask = final_data['top_emotion'].notnull()
final_data = final_data[mask]

# Tokenization
tokenizer, sequences = tokenize_tweets(final_data['Tweet'])
X = pad_sequences(sequences, maxlen=280)
y = final_data['top_emotion_int'].values



# One-Hot Encoding
encoder = LabelBinarizer()
y_encoded = encoder.fit_transform(y)

# Train-Test-Validation Split
X_train, X_temp, y_train_encoded, y_temp_encoded = train_test_split(X, y_encoded, test_size=0.4, stratify=y_encoded)
X_val, X_test, y_val_encoded, y_test_encoded = train_test_split(X_temp, y_temp_encoded, test_size=0.5, stratify=y_temp_encoded)

# Embedding Matrix
embedding_matrix = create_embedding_matrix(tokenizer, embeddings)



# Build and train model
num_classes = y_encoded.shape[1]
model = build_lstm_model(embedding_matrix, num_classes)

# Train and evaluate
history = train_and_evaluate(model, X_train, y_train_encoded, X_val, y_val_encoded)

# Model evaluation on test set
y_pred = model.predict(X_test).argmax(axis=1)
y_test_labels = y_test_encoded.argmax(axis=1)
print("Accuracy on test set:", accuracy_score(y_test_labels, y_pred))
print("\nClassification Report:\n", classification_report(y_test_labels, y_pred))

label_counts = final_data['top_emotion'].value_counts()
print(label_counts)

# Plotting Results
# Summarize the history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Summarize for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

accuracy = accuracy_score(y_test_labels, y_pred)
print("Accuracy:", accuracy)
#saving our model
model.save("lstm2_model.h5")

import pickle

#saving our tokenizer
with open('lstm2_tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

#Testing our model on new data

# Convert integer predictions back to 2D one-hot encoded form
new_preds_2D = np.zeros((1, num_classes))
new_preds_2D[0, new_preds] = 1

# Use inverse_transform to get the labels back
new_labels = encoder.inverse_transform(new_preds_2D)
print("Predicted Labels:", new_labels)

from tensorflow.keras.models import load_model

model = load_model("/content/lstm2_model.h5")

with open('/content/lstm2_tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

# Replace with your own tweets


# Import necessary libraries
from keras.models import load_model
from keras.preprocessing.text import Tokenizer
import numpy as np
from sklearn.preprocessing import OneHotEncoder

# Load the saved model and tokenizer
model = load_model('/content/lstm2_model.h5')


#  new tweets or input data
new_tweets = ["MY LIFE IS GOING DOWN", "I AM TIRED OF LIFE"]
# Convert these tweets into sequences
new_sequences = tokenizer.texts_to_sequences(new_tweets)

# Pad sequences
new_padded_seqs = pad_sequences(new_sequences, maxlen=280, padding='post', truncating='post')

# Predict the class for each tweet using the loaded model
new_preds = model.predict(new_padded_seqs).argmax(axis=1)

# Now, converting predictions to original labels:
# First, create a dummy identity matrix where each row is an one-hot encoded vector
new_preds_onehot = np.eye(num_classes)[new_preds]

# Use the encoder's inverse_transform method to convert one-hot encoded predictions back to original labels
new_labels = encoder.inverse_transform(new_preds_onehot)

print("Predicted Labels:", new_labels)

# Loading saved model
model = load_model('/content/lstm2_model.h5')

# Loading saved tokenizer
import pickle
with open('/content/lstm2_tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)


# New tweets to predict
new_tweets = [
    "I love the new design of this app!",
    "Why is my account constantly crashing?"
]

# Tokenize and pad the new tweets
new_tokenized = tokenizer.texts_to_sequences(new_tweets)
new_padded_seqs = pad_sequences(new_tokenized, maxlen=100, padding='post', truncating='post')

# Predict the classes for the tweets
new_preds = model.predict(new_padded_seqs).argmax(axis=1)

# Convert integer predictions back to original emotion labels
new_preds_reshaped = new_preds.reshape(-1, 1)
new_labels = encoder.inverse_transform(new_preds_reshaped)

# Print the predictions
for tweet, label in zip(new_tweets, new_labels):
    print(f"Tweet: {tweet}\nPredicted Emotion: {label[0]}\n")

from sklearn.preprocessing import LabelEncoder

# Assuming `labels` is the list/array of all our labels from the training data.
 #just the unique labels 
labels = ['negative',
'anger',
'anticipation',
'positive',
'fear',
'joy ',
'neutral',
'disgust',
'sadness',
'trust ',
'surprise']  # Replace with your actual labels

# Recreate the encoder
encoder = LabelEncoder()
encoder.fit(labels)



# Convert integer predictions back to original emotion labels
new_preds_reshaped = new_preds.reshape(-1, 1)
new_labels = encoder.inverse_transform(new_preds_reshaped.ravel())

# Print the predictions
for tweet, label in zip(new_tweets, new_labels):
    print(f"Tweet: {tweet}\nPredicted Emotion: {label}\n")