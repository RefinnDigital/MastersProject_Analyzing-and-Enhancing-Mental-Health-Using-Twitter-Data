# -*- coding: utf-8 -*-
"""BERTMODEL_dissertation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uIdTF8LU8O-JPeJ769SCa4JckVH0TIl5

# SETUP ENVIRONMENT
"""

import torch

num_gpus = torch.cuda.device_count()
if num_gpus:
    for i in range(num_gpus):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
else:
    print("No GPUs detected.")

!pip install tqdm

! pip install tweepy
! pip install NRCLex
! pip install preprocessor
! pip install textblob
! pip install transformers
! pip install pycaret
! pip install langdetect

import csv
import re
import string
import pickle
import numpy as np
import pandas as pd
import nltk
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup
from textblob.blob import TextBlob
from nrclex import NRCLex
from numpy import array, asarray, zeros
from tweepy import *
from keras.preprocessing.text import one_hot, Tokenizer
from keras.utils import pad_sequences
from keras.models import Sequential
from keras.layers import Activation, Dropout, Dense, Flatten, GlobalMaxPooling1D, Embedding, LSTM
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from keras.optimizers import Nadam
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tqdm import tqdm
import pycaret
import transformers
from transformers import AutoModel, BertTokenizerFast
import torch
import torch.nn as nn

# specify GPU
device = torch.device("cuda")

# NLTK downloads
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')  # download wordnet corpus for lemmatization

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

"""# LOAD DATASET

"""

FINAL_DATA = pd.read_csv("/content/SOCIOMENTAL_DATA.csv")

FINAL_DATA.head()

FINAL_DATA.drop_duplicates(inplace=True)

FINAL_DATA.shape

FINAL_DATA.info()

"""# PREPROCESSING"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup

nltk.download('wordnet')
nltk.download('stopwords')

def clean_text(text, lemmatizer=WordNetLemmatizer(), stop_words=set(stopwords.words('english'))):
    # Removing HTML tags

    soup = BeautifulSoup(text, "html.parser")
    no_html_review = soup.get_text().lower()

    abbreviation_dict = {
        "dm": "direct message",
        "pm": "private message",
        "smh": "shaking my head",
        "tbh": "to be honest",
        "idk": "I don't know",
        "omg": "oh my god",
        "lol": "laughing out loud",
        "ofc": "of course",
        "fr": "for real",
        "ppl": "people",
        "rn": "right now"
    }

    # Tokenization and abbreviation replacement
    stop_words = set(stopwords.words('english'))
    stop_words.update(("if","fill","make","thu","every","find"))
    words = word_tokenize(no_html_review)
    words = [abbreviation_dict.get(word, word) for word in words]

    # Lemmatizing words and filtering
    clean_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word.isalpha()]

    return " ".join(clean_words)

# Drop rows where 'Tweet' is not a string
FINAL_DATA = FINAL_DATA[FINAL_DATA['Tweet'].apply(lambda x: isinstance(x, str))]

FINAL_DATA = FINAL_DATA.applymap(lambda x: x.lower() if isinstance(x, str) else x)

FINAL_DATA['Tweet'] =FINAL_DATA['Tweet'].str.replace('rt','co' '')

# Commented out IPython magic to ensure Python compatibility.
# code for cleaning dataset appended in the list above
# %time
FINAL_DATA['Tweet'] = FINAL_DATA['Tweet'].apply(clean_text)

FINAL_DATA.head(2)

FINAL_DATA['Tweet'] =FINAL_DATA['Tweet'].str.replace('co',  '')

FINAL_DATA['Tweet']

FINAL_DATA.head()

FINAL_DATA.isnull().values.any()

FINAL_DATA.info()

"""### SENTIMENT ANALYSIS

### EMOTIONAL LEXICONS
"""

# Function to retrieve the emotion with the highest score
def get_top_emotion(emotion_object):
    emotions = emotion_object.raw_emotion_scores
    if not emotions:
        return 'neutral'
    else:
        return max(emotions, key=emotions.get)

# Creating a new column in the DataFrame to hold the NRCLex sentiment analysis
FINAL_DATA['emotions'] = FINAL_DATA['Tweet'].apply(lambda x: NRCLex(x))

# Getting the top emotion for each tweet
FINAL_DATA['top_emotion'] = FINAL_DATA['emotions'].apply(get_top_emotion)

# Getting the fraction of each emotional type in each tweet for further analysis
FINAL_DATA['emotion_fractions'] = FINAL_DATA['emotions'].apply(lambda x: x.affect_frequencies)

FINAL_DATA.head()

emotional_count = FINAL_DATA["top_emotion"].value_counts()

emotional_count

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Load your data
# DF['top_emotion'] =DF['top_emotion']  # This line is not needed

# Define your classes
classes = [
    "anger",
    "negative",
    "positive",
    "fear",
    "anticipation",
    "sadness",
    "joy",
    "disgust",
    "trust",
    "surprise",
    "neutral"
]

# Generate a word cloud for each class
for class_ in classes:
    # Filter the data for the current class
    class_data = FINAL_DATA[FINAL_DATA['top_emotion'] == class_]

    # Combine all the tweets into a single text
    text = ' '.join(class_data['Tweet'])

    # Generate the word cloud
    wordcloud = WordCloud(width=800, height=400).generate(text)

    # Plot the word cloud
    plt.figure(figsize=(10, 7))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Word Cloud for {class_}')
    plt.show()

"""# BERT MODEL"""

# Constants
MAXLEN = 280
BATCH_SIZE =128
EPOCHS = 20
LEARNING_RATE = 1e-5
STOPPING_PATIENCE = 3
GRADIENT_ACCUMULATION_STEPS = 2

torch.cuda.empty_cache()

final_data = FINAL_DATA

#Ensure your labels are correctly encoded as integers.
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
final_data['top_emotion_int'] = encoder.fit_transform(final_data['top_emotion'])

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokens = final_data['Tweet'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=MAXLEN, truncation=True))
padded_tokens = pad_sequences(tokens, maxlen=MAXLEN, dtype="long", truncating="post", padding="post")
attention_masks = [[float(i != 0) for i in token] for token in padded_tokens]

# Split the data into train+validation and test sets
train_val_inputs, test_inputs, train_val_labels, test_labels, train_val_masks, test_masks = train_test_split(
    padded_tokens, final_data['top_emotion_int'].values, attention_masks, random_state=42, test_size=0.2)

# Split the train+validation set into training and validation sets
train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(
train_val_inputs, train_val_labels, train_val_masks, random_state=42, test_size=0.2)

# Convert to PyTorch tensors
train_inputs = torch.tensor(train_inputs)
validation_inputs = torch.tensor(validation_inputs)
train_labels = torch.tensor(train_labels)
validation_labels = torch.tensor(validation_labels)
train_masks = torch.tensor(train_masks)
validation_masks = torch.tensor(validation_masks)

# Create DataLoader
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=BATCH_SIZE)
validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_dataloader = DataLoader(validation_data, sampler=SequentialSampler(validation_data), batch_size=BATCH_SIZE)

num_labels = len(final_data['top_emotion_int'].unique())

num_labels

# Adjust the BERT model's output size (`num_labels`) to match the number of classes.
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)

final_data.head()

# Create a subset to display the two columns
emotion_class_df = final_data[['top_emotion', 'top_emotion_int']].drop_duplicates()

print(emotion_class_df)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

def train(model, dataloader, optimizer, device):
    model.train()
    total_loss = 0
    total_correct = 0
    optimizer.zero_grad()

    for step, batch in enumerate(tqdm(dataloader, desc="Training Batches")):
        input_ids, attention_mask, labels = batch
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        labels = labels.to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()

        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:
            optimizer.step()
            optimizer.zero_grad()

        logits = outputs.logits
        preds = torch.argmax(logits, dim=1)
        total_correct += (preds == labels).sum().item()

    optimizer.step()

    accuracy = total_correct / len(dataloader.dataset)
    return total_loss / len(dataloader), accuracy

def evaluate(model, dataloader, device):
    model.eval()
    total_loss = 0
    total_correct = 0
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluation Batches"):
            input_ids, attention_mask, labels = batch
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            labels = labels.to(device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits

            total_loss += loss.item()
            all_preds.extend(logits.detach().cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            preds = torch.argmax(logits, dim=1)
            total_correct += (preds == labels).sum().item()

    accuracy = total_correct / len(dataloader.dataset)
    average_loss = total_loss / len(dataloader)
    return average_loss, accuracy, all_preds, all_labels

import matplotlib.pyplot as plt

best_valid_loss = float('inf')
patience_counter = 0

# Lists to store metrics for plotting
train_losses = []
train_accuracies = []
valid_losses = []
valid_accuracies = []

for epoch in range(EPOCHS):
    train_loss, train_acc = train(model, train_dataloader, optimizer, device)
    valid_loss, valid_acc, _, _ = evaluate(model, validation_dataloader, device)

    # Append values for plotting
    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    valid_losses.append(valid_loss)
    valid_accuracies.append(valid_acc)

    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'best_model.pt')
        patience_counter = 0
    else:
        patience_counter += 1

    print(f"\n Epoch: {epoch+1:02}")
    print(f"\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%")
    print(f"\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%")

    if patience_counter >= STOPPING_PATIENCE:
        print("Early stopping")
        break

# Plotting
# Accuracy plot
plt.figure(figsize=(10, 5))
plt.plot(train_accuracies, label="Train Accuracy")
plt.plot(valid_accuracies, label="Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Model Accuracy")
plt.show()

# Loss plot
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label="Train Loss")
plt.plot(valid_losses, label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.title("Model Loss")
plt.show()

# Evaluation
model.load_state_dict(torch.load('best_model.pt'))
valid_loss, valid_acc, all_preds, all_labels = evaluate(model, validation_dataloader, device)
preds_flat = np.argmax(all_preds, axis=1).flatten()
labels_flat = np.array(all_labels).flatten()
print(classification_report(labels_flat, preds_flat))

print(f"Final Test Accuracy: {valid_acc*100:.2f}%")

print(f"Final Training Accuracy: {train_acc*100:.2f}%")



from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, f1_score, accuracy_score

# Confusion Matrix
cm = confusion_matrix(labels_flat, preds_flat)
print("Confusion Matrix:\n", cm)

all_preds = np.array(all_preds)

import seaborn as sns
import matplotlib.pyplot as plt

def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

import itertools

# Compute confusion matrix
cm = confusion_matrix(labels_flat, preds_flat)

# Plot non-normalized confusion matrix
plt.figure(figsize=(8, 8))
plot_confusion_matrix(cm, classes=encoder.classes_ )

plt.show()

# Save model
model.save_pretrained('path/to/save/bert_model')

# Save tokenizer
tokenizer.save_pretrained('path/to/save/bert_tokenizer')

# Save model
torch.save(model.state_dict(), '/content/best_model.pt')

# Load model
model = BertForSequenceClassification.from_pretrained('path/to/save/bert_model')

# Load tokenizer
tokenizer = BertTokenizer.from_pretrained('path/to/save/bert_tokenizer')

import pickle

# Save model and tokenizer to file
with open('bert_TOKENIZER.pkl', 'wb') as f:
    pickle.dump(model, f)

# Preprocess the unseen data
unseen_data = ["what is life without you mom", "i feel so lonely without you mother"]
inputs = tokenizer(unseen_data, return_tensors='pt', truncation=True, padding=True, max_length=MAXLEN)
inputs = inputs.to(device)
# Make predictions
with torch.no_grad():
    outputs = model(**inputs)
probs = torch.nn.functional.softmax(outputs.logits, dim=-1)

# Get the predicted labels
predictions = torch.argmax(probs, dim=1)

# Convert the predicted labels to class names
predicted_classes = [encoder.classes_[i] for i in predictions]

print(predicted_classes)

